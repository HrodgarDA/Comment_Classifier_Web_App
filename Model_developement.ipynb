{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f1eac10",
      "metadata": {
        "id": "1f1eac10"
      },
      "source": [
        "## Project: Toxic Comment Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3504d2cc",
      "metadata": {
        "id": "3504d2cc"
      },
      "source": [
        "Build a model that can filter user comments according to the degree of harmfulness of the language:\n",
        "1. Preprocess the text by eliminating the set of tokens that do not make significant contribution at the semantic level\n",
        "2. Transform the text corpus into sequences\n",
        "3. Build a Deep Learning model including recurrent layers for a multilabel classification task\n",
        "4. At prediction time, the model must return a vector containing a 1 or a 0 at each label in the dataset (toxic, severe_toxic, obscene, threat, insult, identity_hate). In this way, a non-harmful comment will be classified by a vector of only 0s [0,0,0,0,0]. In contrast, a dangerous comment will exhibit at least a 1 among the 6 labels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I1t_lCPmWbqx",
      "metadata": {
        "id": "I1t_lCPmWbqx"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6EuEghOu-DZw",
      "metadata": {
        "id": "6EuEghOu-DZw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2f2fb60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2f2fb60",
        "outputId": "0295f6f2-716f-4f1a-f522-b91c6ad33a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (159571, 8)\n",
            "                                         comment_text  toxic  severe_toxic  \\\n",
            "0   Explanation\\nWhy the edits made under my usern...      0             0   \n",
            "1   D'aww! He matches this background colour I'm s...      0             0   \n",
            "2   Hey man, I'm really not trying to edit war. It...      0             0   \n",
            "3   \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
            "4   You, sir, are my hero. Any chance you remember...      0             0   \n",
            "5   \"\\n\\nCongratulations from me as well, use the ...      0             0   \n",
            "6        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1             1   \n",
            "7   Your vandalism to the Matt Shirvington article...      0             0   \n",
            "8   Sorry if the word 'nonsense' was offensive to ...      0             0   \n",
            "9   alignment on this subject and which are contra...      0             0   \n",
            "10  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...      0             0   \n",
            "11  bbq \\n\\nbe a man and lets discuss it-maybe ove...      0             0   \n",
            "12  Hey... what is it..\\n@ | talk .\\nWhat is it......      1             0   \n",
            "13  Before you start throwing accusations and warn...      0             0   \n",
            "14  Oh, and the girl above started her arguments w...      0             0   \n",
            "15  \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...      0             0   \n",
            "16  Bye! \\n\\nDon't look, come or think of comming ...      1             0   \n",
            "17   REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski      0             0   \n",
            "18  The Mitsurugi point made no sense - why not ar...      0             0   \n",
            "19  Don't mean to bother you \\n\\nI see that you're...      0             0   \n",
            "20  \"\\n\\n Regarding your recent edits \\n\\nOnce aga...      0             0   \n",
            "21  \"\\nGood to know. About me, yeah, I'm studying ...      0             0   \n",
            "22  \"\\n\\n Snowflakes are NOT always symmetrical! \\...      0             0   \n",
            "23  \"\\n\\n The Signpost: 24 September 2012 \\n\\n Rea...      0             0   \n",
            "24  \"\\n\\nRe-considering 1st paragraph edit?\\nI don...      0             0   \n",
            "\n",
            "    obscene  threat  insult  identity_hate  sum_injurious  \n",
            "0         0       0       0              0              0  \n",
            "1         0       0       0              0              0  \n",
            "2         0       0       0              0              0  \n",
            "3         0       0       0              0              0  \n",
            "4         0       0       0              0              0  \n",
            "5         0       0       0              0              0  \n",
            "6         1       0       1              0              4  \n",
            "7         0       0       0              0              0  \n",
            "8         0       0       0              0              0  \n",
            "9         0       0       0              0              0  \n",
            "10        0       0       0              0              0  \n",
            "11        0       0       0              0              0  \n",
            "12        0       0       0              0              1  \n",
            "13        0       0       0              0              0  \n",
            "14        0       0       0              0              0  \n",
            "15        0       0       0              0              0  \n",
            "16        0       0       0              0              1  \n",
            "17        0       0       0              0              0  \n",
            "18        0       0       0              0              0  \n",
            "19        0       0       0              0              0  \n",
            "20        0       0       0              0              0  \n",
            "21        0       0       0              0              0  \n",
            "22        0       0       0              0              0  \n",
            "23        0       0       0              0              0  \n",
            "24        0       0       0              0              0  \n"
          ]
        }
      ],
      "source": [
        "BASE_URL = \"https://s3.eu-west-3.amazonaws.com/profession.ai/datasets/\"\n",
        "df = pd.read_csv(BASE_URL+\"Filter_Toxic_Comments_dataset.csv\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(df.head(25))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hnku2LwKWiby",
      "metadata": {
        "id": "Hnku2LwKWiby"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uInqGdtiCWgJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uInqGdtiCWgJ",
        "outputId": "a17be6be-3a2c-418e-a7b5-1dabbd20cb1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):  #lower case conversion, Removal of special characters and stopwords\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    return text\n",
        "\n",
        "df['cleaned_comment'] = df['comment_text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TcFoM_ahCWic",
      "metadata": {
        "id": "TcFoM_ahCWic"
      },
      "outputs": [],
      "source": [
        "MAX_WORDS = 10000\n",
        "MAX_LENGTH = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer.fit_on_texts(df['cleaned_comment'])\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_comment'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_LENGTH)\n",
        "\n",
        "label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] # Class labels\n",
        "y = df[label_columns].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2UQpYK8WpvH",
      "metadata": {
        "id": "b2UQpYK8WpvH"
      },
      "source": [
        "### Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vihl8ysykeCJ",
      "metadata": {
        "id": "Vihl8ysykeCJ"
      },
      "source": [
        "*   LSTM layers are effective in capturing long-term dependencies in the text, which are crucial for understanding the context and tone of the commentary. The two-level structure allows deeper processing of sequential features.\n",
        "\n",
        "*   The fully connected layer with 64 neurons and ReLU activation introduces nonlinearity into the model.\n",
        "\n",
        "\n",
        "*   The 50% dropout helps prevent overfitting, increasing the generalization of the model.\n",
        "\n",
        "*   Final dense layer with 6 neurons (one for each class) and sigmoid activation, sigmoid is appropriate for multi-label classification, producing independent probabilities for each class.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93EQsDFoCWk1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "93EQsDFoCWk1",
        "outputId": "52f41b75-d186-4362-8b1c-bdbcce4dbea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m1,280,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m49,408\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m12,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m2,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m390\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,344,326\u001b[0m (5.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344,326</span> (5.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,344,326\u001b[0m (5.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344,326</span> (5.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Embedding(MAX_WORDS, 128, input_length=MAX_LENGTH),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(6, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', 'mse'])\n",
        "\n",
        "model.build(input_shape=(None, MAX_LENGTH))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rRYGweVO0cyG",
      "metadata": {
        "id": "rRYGweVO0cyG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F_L1BgNwCWnX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_L1BgNwCWnX",
        "outputId": "4f87840b-0e7d-47b8-9227-84555277256c"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m798/798\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 635ms/step - accuracy: 0.2191 - loss: 0.3616 - mse: 0.1112 - val_accuracy: 0.9943 - val_loss: 0.1417 - val_mse: 0.0344\n",
            "Epoch 2/10\n",
            "\u001b[1m798/798\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 620ms/step - accuracy: 0.5693 - loss: 0.1604 - mse: 0.0377 - val_accuracy: 0.9835 - val_loss: 0.0926 - val_mse: 0.0254\n",
            "Epoch 3/10\n",
            "\u001b[1m798/798\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572ms/step - accuracy: 0.6164 - loss: 0.0888 - mse: 0.0227"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction function definition\n",
        "def predict_toxicity(text):\n",
        "    cleaned = preprocess_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([cleaned])\n",
        "    padded = pad_sequences(seq, maxlen=MAX_LENGTH)\n",
        "    prediction = model.predict(padded)\n",
        "    binary_prediction = (prediction > 0.5).astype(int)[0]\n",
        "    classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] # Lista delle classi\n",
        "\n",
        "    positive_classes = [classes[i] for i, value in enumerate(binary_prediction) if value == 1]\n",
        "\n",
        "    if positive_classes == []:\n",
        "      print(\"The text sample provided is approved\")\n",
        "    else:\n",
        "        print(f\"The text sample provided is classified as: {positive_classes}\")\n",
        "\n",
        "    return binary_prediction, positive_classes"
      ],
      "metadata": {
        "id": "80tR3aYymcpK"
      },
      "id": "80tR3aYymcpK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5tyeZ3BIlkLx",
      "metadata": {
        "id": "5tyeZ3BIlkLx"
      },
      "source": [
        "### Model and tokenizer export"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8JUC7H2m7RZg"
      },
      "id": "8JUC7H2m7RZg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a directory for your project in Google Drive\n",
        "project_dir = '/content/drive/My Drive/Colab Notebooks/model_parameters'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "model.save(os.path.join(project_dir, 'RNN_model.h5'))\n",
        "\n",
        "# Save the tokenizer\n",
        "import pickle\n",
        "with open(os.path.join(project_dir, 'tokenizer.pickle'), 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Save configuration\n",
        "import json\n",
        "config = {\n",
        "    'MAX_WORDS': MAX_WORDS,\n",
        "    'MAX_LENGTH': MAX_LENGTH,\n",
        "    'label_columns': label_columns\n",
        "}\n",
        "with open(os.path.join(project_dir, 'config.json'), 'w') as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "print(f\"All files saved in Google Drive: {project_dir}\")"
      ],
      "metadata": {
        "id": "vCMmBrmFw6zu"
      },
      "id": "vCMmBrmFw6zu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9UcPZkc1e-Ny",
      "metadata": {
        "id": "9UcPZkc1e-Ny"
      },
      "outputs": [],
      "source": [
        "sample_text = \"You are an idiot\"\n",
        "result = predict_toxicity(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NhikSe1OfY6B",
      "metadata": {
        "id": "NhikSe1OfY6B"
      },
      "outputs": [],
      "source": [
        "sample_text = \"Good job bro, keep it up!\"\n",
        "result = predict_toxicity(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AXMuk4FoiSPw",
      "metadata": {
        "id": "AXMuk4FoiSPw"
      },
      "outputs": [],
      "source": [
        "sample_text = \"I've never seen such a bad movie!\"\n",
        "result = predict_toxicity(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MlYLEh12iXGX",
      "metadata": {
        "id": "MlYLEh12iXGX"
      },
      "outputs": [],
      "source": [
        "sample_text = \"The pasta was horrible, I'll never go there again\"\n",
        "result = predict_toxicity(sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rK_Cp0XqOwrl",
      "metadata": {
        "id": "rK_Cp0XqOwrl"
      },
      "source": [
        "### TRY WITH YOUR SENTENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZhYhNdLyiklm",
      "metadata": {
        "id": "ZhYhNdLyiklm"
      },
      "outputs": [],
      "source": [
        "sentence = input(\"Enter a sentence: \")\n",
        "result = predict_toxicity(sentence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rK_Cp0XqOwrl"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}